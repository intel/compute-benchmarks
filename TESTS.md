# api_overhead_benchmark
Api Overhead Benchmark is a set of tests aimed at measuring CPU-side execution duration of compute API calls.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
AppendLaunchKernel|measures time spent in zeCommandListAppendLaunchKernel on CPU.|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size, pass 0 to make the driver calculate it during enqueue</li></ul>|:heavy_check_mark:|:x:|
AppendWaitOnEventsImmediate|Measures time spent to zeCommandListAppendWaitOnEvents using immediate command list.|<ul><li>--eventSignaled Event is already signaled before zeCommandListAppendWaitOnEvents call (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
CreateBuffer|measures time spent in clCreateBuffer on CPU.|<ul><li>--allocateAll Free buffers at the end of test, as opposed to freeing between iterations. Should disallow resource reuse (0 or 1)</li><li>--bufferSize Buffer size</li><li>--copyHostPtr CL_MEM_COPY_HOST_PTR flag (0 or 1)</li><li>--forceHostMemoryIntel CL_MEM_FORCE_HOST_MEMORY_INTEL flag (0 or 1)</li><li>--readOnly Read only buffer (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CreateCommandList|measures time spent in zeCommandListCreate on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li><li>--CopyOnly Create copy only cmdlist (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
CreateCommandListImmediate|measures time spent in zeCommandListCreateImmediate on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li></ul>|:heavy_check_mark:|:x:|
DestroyCommandList|measures time spent in zeCommandListDestroy on CPU.|<ul><li>--CmdListCount Number of cmdlists to destroy</li></ul>|:heavy_check_mark:|:x:|
DestroyCommandListImmediate|measures time spent in zeCommandListDestroy on CPU, for immediate cmdlist.|<ul><li>--CmdListCount Number of immediate cmdlists to create</li></ul>|:heavy_check_mark:|:x:|
DriverGet|measures time spent in driver get call on CPU.|<ul><li>--getDriverCount Whether to measure driver count or driver get (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
DriverGetApiVersion|measures time spent in zeDriverGetApiVersion call on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
DriverGetProperties|measures time spent in zeDriverGetProperties call on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
EnqueueNdrNullLws|measures time spent in clEnqueueNDRangeKernel on CPU. Null LWS is provided, which causes driver to calculate it|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--gws Global work size</li><li>--ooq Use out of order queue (0 or 1)</li><li>--profiling Creating a profiling queue (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
EnqueueNdrTime|measures time spent in clEnqueueNDRangeKernel on CPU.|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--ooq Use out of order queue (0 or 1)</li><li>--profiling Creating a profiling queue (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size</li></ul>|:x:|:heavy_check_mark:|
EventCreation|measures time spent to create event|<ul><li>--hostVisible Event will set host visible flag (0 or 1)</li><li>--signal Type of signal scope (subdevice or device or host or none)</li><li>--useProfiling Event will use profiling (0 or 1)</li><li>--wait Type of wait scope (subdevice or device or host or none)</li></ul>|:heavy_check_mark:|:x:|
EventQueryStatus|Measures time spent to query event status|<ul><li>--eventSignaled Event will be set as signaled (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandList|measures time spent in zeCommandQueueExecuteCommandLists on CPU.|<ul><li>--UseFence Pass a non-null ze_fence_handle_t to the API call (0 or 1)</li><li>--measureCompletionTime Measures time taken to complete the submission (default is to measure only Execute call) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListForCopyEngine|measures CPU time spent in zeCommandQueueExecuteCommandLists for copy-only path|<ul><li>--UseFence Pass a non-null ze_fence_handle_t to the API call (0 or 1)</li><li>--measureCompletionTime Measures time taken to complete the submission (default is to measure only Execute call) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListImmediate|measures time spent in appending launch kernel for immediate command list on CPU.|<ul><li>--CallsCount amount of calls that is being meassured</li><li>--KernelExecutionTime How long a single kernel executes, in us</li><li>--MeasureCompletionTime Measures time taken to complete the submission (default is to measure only Immediate call) (0 or 1)</li><li>--UseProfiling Pass a profiling ze_event_t to the API call (0 or 1)</li><li>--useBarrierSynchronization Uses barrier synchronization instead of waiting for event from last kernel (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListImmediateCopyQueue|measures time spent in appending memory copy for immediate command list on CPU with Copy Queue.|<ul><li>--IsCopyOnly If true, Copy Engine is selected. If false, Compute Engine is selected (0 or 1)</li><li>--MeasureCompletionTime Measures time taken to complete the submission (default is to measure only Immediate call) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListImmediateMultiKernel|measures time spent in executing multiple instances of two different kernels with immediate command list on CPU|<ul><li>--AddBarrier Add a Barrier after certain number of Kernel launches, number of kernels before barrier is controlled by numKernelsBeforeBarrier (0 or 1)</li><li>--CallsCount amount of calls that is being measured</li><li>--KernelExecutionTime How long a single kernel executes, in us</li><li>--NumKernelsAfterBarrier Adds certain number of kernels after Barrier, Default is 2</li><li>--NumKernelsBeforeBarrier Adds certain number of kernels prior to Barrier, Default is 2</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceCreate|measures time spent in zeFenceCreate on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceDestroy|measures time spent in zeFenceDestroy on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceUsage|measures time spent in zeCommandQueueExecuteCommandLists and zeFenceSynchronize on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithIndirectAccess|measures time spent in zeCommandQueueExecuteCommandLists on CPU when indirect allocations are accessed.|<ul><li>--AmountOfIndirectAllocations Amount of indirect allocations that are present in system</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithIndirectArguments|measures time spent in zeCommandQueueExecuteCommandLists on CPU when indirect allocations are used.|<ul><li>--AmountOfIndirectAllocations Amount of indirect allocations that are present in system</li><li>--placement Placement of the indirect allocations (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li></ul>|:heavy_check_mark:|:x:|
FlushTime|measures time spent in clEnqueueNDRangeKernel on CPU.|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--ooq Use out of order queue (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size, pass 0 to make the driver calculate it during enqueue</li></ul>|:x:|:heavy_check_mark:|
KernelSetArgumentValueImmediate|measures time spent in zeKernelSetArgumentValue for immediate arguments on CPU.|<ul><li>--argSize Kernel argument size in bytes</li></ul>|:heavy_check_mark:|:x:|
LifecycleCommandList|measures time spent in zeCommandListCreate + Close + Execute on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li><li>--CopyOnly Create copy only cmdlist (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ResetCommandList|measures time spent in zeCommandListReset on CPU.|<ul><li>--CopyOnly Create copy only cmdlist (0 or 1)</li><li>--size Size of the buffer</li><li>--sourcePlacement Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li></ul>|:heavy_check_mark:|:x:|
SetKernelArgSvmPointer|measures time spent in clSetKernelArgSVMPointer on CPU.|<ul><li>--allocationSize Size of svm allocations, in bytes</li><li>--allocationsCount Number of allocations</li><li>--reallocate Allocations will be freed and allocated again between setKernelArgs (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
SetKernelGroupSize|measures time spent in zeKernelSetGroupSize on CPU.|<ul><li>--workDim Number of dimensions</li></ul>|:heavy_check_mark:|:x:|
UsmMemoryAllocation|measures time spent in USM memory allocation APIs.|<ul><li>--size Size to allocate</li><li>--type Type of memory being allocated (Device or Host or Shared)</li></ul>|:heavy_check_mark:|:x:|



# atomic_benchmark
Atomic Benchmark is a set of tests aimed at measuring performance of atomic operations inside kernels.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
OneAtomic|enqueues kernel performing an atomic operation on a single address|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneAtomicExplicit|enqueues kernel performing an atomic operation on a single address using OpenCL 2.0 Atomics with explicit memory order and scope|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneLocalAtomic|enqueues kernel performing an atomic operation on a single location placed in SLM|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneLocalAtomicExplicit|enqueues kernel performing an atomic operation on a single location placed in SLM using OpenCL 2.0 Atomics with explicit memory order and scope|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
SeparateAtomics|enqueues kernel performing an atomic operation on different addresses|<ul><li>--atomicsPerCacheline Number of used addresses occupying a single cacheline (this causes operations to be serialized)</li><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
SeparateAtomicsExplicit|enqueues kernel performing an atomic operation on different addresses|<ul><li>--atomicsPerCacheline Number of used addresses occupying a single cacheline (this causes operations to be serialized)</li><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|



# eu_benchmark
EU Benchmark is a set of tests aimed at measuring performance of calculations performed in kernels.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
DoMathOperation|enqueues kernel performing a math operation|<ul><li>--op Math operation to perform (Add or Sub or Div or Modulo or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
ReadAfterAtomicWrite|enqueues kernel, which writes to global memory using atomic and then reads non atomically|<ul><li>--atomic If true, write to global memory will be atomic. (0 or 1)</li><li>--shuffleRead If true, each thread will write and read different memory cell. Otherwise it will be the same one. (0 or 1)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgs Workgroup size</li></ul>|:x:|:heavy_check_mark:|



# gpu_cmds_benchmark
Gpu Commands Benchmark is a set of tests aimed at measuring GPU-side execution duration of various commands.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
BarrierBetweenKernels|measures time required to run a barrier command between 2 kernels, including potential cache flush commands|<ul><li>--bytes bytes to flush from L3</li><li>--memoryType memory type cached in L3 (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--onlyReads only reads cached in L3</li><li>--remoteAccess access cached from remote tile</li></ul>|:heavy_check_mark:|:x:|
CopyWithEvent|measures time required to run a copy kernel with various event configurations.|<ul><li>--devWaitEvent Use ZE_EVENT_SCOPE_FLAG_DEVICE for ze_event_desc_t::wait (0 or 1)</li><li>--hostSignalEvent Use ZE_EVENT_POOL_HOST_VISIBLE for ze_event_pool_desc_t::flags, and use ZE_EVENT_SCOPE_FLAG_HOST for ze_event_desc_t::signal (0 or 1)</li><li>--measuredCmds Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--timestampEvent Use ZE_EVENT_POOL_FLAG_KERNEL_TIMESTAMP for ze_event_pool_desc_t::flags (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
EmptyKernel|measures time required to run an empty kernel on GPU.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
EventCtxtSwitchLatency|measures context switching latency time required to switch between various engine types|<ul><li>--firstEngine first engine to measure context switch latency (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--secondEngine second engine to measure context switch latency (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li></ul>|:heavy_check_mark:|:x:|
KernelWithEvent|measures time required to run an empty kernel with various event configurations.|<ul><li>--devWaitEvent Use ZE_EVENT_SCOPE_FLAG_DEVICE for ze_event_desc_t::wait (0 or 1)</li><li>--hostSignalEvent Use ZE_EVENT_POOL_HOST_VISIBLE for ze_event_pool_desc_t::flags, and use ZE_EVENT_SCOPE_FLAG_HOST for ze_event_desc_t::signal (0 or 1)</li><li>--measuredCmds Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--timestampEvent Use ZE_EVENT_POOL_FLAG_KERNEL_TIMESTAMP for ze_event_pool_desc_t::flags (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
KernelWithWork|measures time required to run a GPU kernel which assigns values to elements of a buffer.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventCold|measures time required to service a signalled semaphore, that has never been waited for.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventFromWalker|measures time required to service a signalled semaphore coming from Walker command|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventHot|measures time required to service a signalled semaphore, that was previously used|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WriteTimestamp|measures time required to write a timestamp on GPU.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|



# memory_benchmark
Memory Benchmark is a set of tests aimed at measuring bandwidth of memory transfers.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
CopyBuffer|allocates two OpenCL buffers and measures copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--compressedDestination Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--compressedSource Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffers (Zeros or Random)</li><li>--size Size of the buffers</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CopyBufferRect|allocates two OpenCL buffers and measures rectangle copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--dstCompressed Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li><li>--srcCompressed Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CopyEntireImage|allocates two image objects and measures copy bandwidth between them. Images will be placed in device memory, if it's available.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the image</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
FillBuffer|allocates an OpenCL buffer and measures fill bandwidth. Buffer will be placed in device memory, if it's available.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
MapBuffer|allocates an OpenCL buffer and measures map bandwidth. Mapping operation means memory transfer from GPU to CPU or a no-op, depending on map flags.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--mapFlags OpenCL map flags passed during memory mapping (Read or Write or WriteInvalidate)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBuffer|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--reuse How hostptr allocation can be reused due to previous operations (None or Usm or Map)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBufferMisaligned|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU. Destination pointer passed by the application will be misaligned by the specified amount of bytes.|<ul><li>--misalignment Number of bytes by which misaligned the destination pointer will be misaligned</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBufferRect|allocates an OpenCL buffer and measures rectangle read bandwidth. Rectangle read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|
ReadDeviceMemBuffer|allocates two OpenCL buffers and measures source buffer read bandwidth. Source buffer resides in device memory.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|
RemoteAccessMemory|Uses stream memory triad to measure bandwidth with different percentages of remote memory access.|<ul><li>--remoteFraction Fraction of remote memory access. 1 / n</li><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--workItemSize Number of work items group together for remote check</li></ul>|:x:|:heavy_check_mark:|
SLM_DataAccessLatency|generates SLM local memory transactions inside thread group to measure latency between reads (uses Intel only private intel_get_cycle_counter() )|<ul><li>--direction write or read mode (0 or 1)</li><li>--occupancyDiv H/W load divider by 8, 4, 2, full occupancy</li><li>--size SLM Size</li></ul>|:x:|:heavy_check_mark:|
SlmSwitchLatency|Enqueues 2 kernels with different SLM size. Measures switch time between these kernels.|<ul><li>--firstSlmSize Size of the shared local memory per thread group. First kernel.</li><li>--secondSlmSize Size of the shared local memory per thread group. Second kernel.</li><li>--wgs Size of the work group.</li></ul>|:heavy_check_mark:|:x:|
StreamAfterTransfer|Goal of this test is to measure how stream kernels perform right after host to device transfer populating the data. Test does clean caches, then emits transfers and then follows with stream kernel and measures GPU execution time of it.|<ul><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
StreamMemory|Streams memory inside of kernel in a fashion described by 'type'. Copy means one memory location is read from and the second one is written to. Triad means two buffers are read and one is written to. In read and write memory is only read or written to.|<ul><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
StreamMemoryImmediate|Streams memory inside of kernel in a fashion described by 'type' using immediate command list. Copy means one memory location is read from and the second one is written to. Triad means two buffers are read and one is written to. In read and write memory is only read or written to.|<ul><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UnmapBuffer|allocates an OpenCL buffer and measures unmap bandwidth. Unmapping operation meansmemory transfer from CPU to GPU or a no-op, depending on map flags.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--mapFlags OpenCL map flags passed during memory mapping (Read or Write or WriteInvalidate)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmCopy|allocates two unified shared memory buffers and measures copy bandwidth between them.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--reuseCmdList Command list is reused between iterations (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyImmediate|allocates two unified shared memory buffers and measures copy bandwidth between them using immediate command list.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyMultipleBlits|allocates two unified shared memory buffers, divides them into chunks, copies each chunk using a different copy engine and measures bandwidth. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--size Size of the operation processed by each engine</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyRegion|TODO|<ul><li>--dstOrigin </li><li>--dstRegion </li><li>--dstptr Placement of the destination buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--reuseCmdList Command list is reused between iterations (0 or 1)</li><li>--srcOrigin </li><li>--srcRegion </li><li>--srcptr Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyStagingBuffers|Measures copy time from device/host to host/device. Host memory is non-USM allocation.Copy is done through staging USM buffers. Non-USM host ptr is never passed to L0 API, only through staging buffers.|<ul><li>--chunks How much memory chunks should the buffer be splitted into</li><li>--dst Memory placement of destination (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:x:|
UsmFill|allocates a unified memory buffer and measures fill bandwidth|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFillImmediate|allocates a unified memory buffer and measures fill bandwidth using immediate command list|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmFillMultipleBlits|allocates a unified shared memory buffer, divides it into chunks, copies each chunk using a different copy engine and measures bandwidth. Refer to UsmCopyMultipleBlits for more details.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--memory Placement of buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the operation processed by each engine</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFillSpecificPattern|allocates a unified memory buffer and measures fill bandwidth. Allow specifying arbitrary pattern.|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--pattern The fill pattern represented hexadecimally, e.g. 0x91ABCD1254</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmImmediateCopyMultipleBlits|allocates two unified shared memory buffers, divides them into chunks, copies each chunk using a different copy engine with an immediate command list and  measures bandwidth. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--size Size of the operation processed by each engine</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li></ul>|:heavy_check_mark:|:x:|
UsmMemset|allocates a unified memory buffer and measures memset bandwidth|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM or non-USM-imported or non-USM-mapped)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmSharedMigrateCpu|allocates a unified shared memory buffer and measures bandwidth for kernel that must migrate resource from GPU to CPU|<ul><li>--accessAllBytes Select, whether entire resource or only one byte will be accessed on CPU (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpu|allocates a unified shared memory buffer and measures bandwidth for kernel that must migrate resource from CPU to GPU|<ul><li>--prefetch Explicitly migrate shared allocation to device associated with command queue (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpuForFill|allocates a unified shared memory buffer and measures bandwidth for memory fill operation that must migrate resource from CPU to GPU|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--prefetch Explicitly migrate shared allocation to device associated with command queue (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteBuffer|allocates an OpenCL buffer and measures write bandwidth. Write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--reuse How hostptr allocation can be reused due to previous operations (None or Usm or Map)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
WriteBufferRect|allocates an OpenCL buffer and measures rectangle write bandwidth. Rectangle write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|



# miscellaneous_benchmark
Miscellaneous Benchmark is a set of tests measuring different simple compute scenarios.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
IoqKernelSwitchLatency|measures time from end of one kernel till start of next kernel for in order queue|<ul><li>--kernelCount Count of kernels</li><li>--useEvents Use events to synchronize between kernels (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
KernelWithWork|measures time required to run a GPU kernel which assigns constant values to elements of a buffer. Each thread assigns one value. Benchmark checks the impact of kernel split.|<ul><li>--split How many times kernel is split</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:x:|:heavy_check_mark:|
Reduction|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction2|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction3|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction4|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction5|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
VectorSum|Performs vector addition|<ul><li>--numberOfElementsX Number of elements in X dimension</li><li>--numberOfElementsY Number of elements in Y dimension</li><li>--numberOfElementsZ Number of elements in Z dimension</li></ul>|:x:|:heavy_check_mark:|



# multithread_benchmark
Multithread Benchmark is a set of tests aimed at measuring how different commands benefit from multithreaded execution.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
SvmCopy|enqueues multiple svm copies on multiple threads concurrently.|<ul><li>--numberOfThreads Number of threads that will run concurrently</li></ul>|:heavy_check_mark:|:heavy_check_mark:|



# multitile_memory_benchmark
Multi-tile Memory Benchmark is a set of tests aimed at measuring bandwidth of memory transfers performed on a multi-tile device.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
CopyBuffer|allocates two OpenCL buffers and measures copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--dstCompressed Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--srcCompressed Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
FillBuffer|allocates an OpenCL buffer and measures fill bandwidth. Buffer will be placed in device memory, if it's available.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--patternSize Size of the fill pattern</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBuffer|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmBidirectionalCopy|allocates two unified device memory buffers, each on a different tile, and measures copy bandwidth between. Test measures copies on two directions, which can be controlled with the -write parameter: with -write=1, each tile performs a write operation. For instance: queue is placed in tile 0, source is buffer in tile 0, and destination is in tile 1. Similarly for tile 1, queue is placed in tile 1, source in tile 1, and destination in tile 0. With -write=0, the destination and source are flipped: queue is placed in tile 0, source is buffer in tile 1, and destination is in tile 0, while for tile 1, queue is placed in tile 1, source in tile 0, and destination in tile 1.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffers</li><li>--write Which operation is used, whether write or read (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopy|allocates two unified shared memory buffers and measures copy bandwidth between them using a builtin function.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyImmediate|allocates two unified shared memory buffers and measures copy bandwidth between them using a builtin function appended to an immediate list.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyKernel|allocates two unified shared memory buffers and measures copy bandwidth between them using a custom kernel.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFill|allocates a unified shared memory buffer and measures fill bandwidth.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--patternSize Size of the fill pattern</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateCpu|allocates a unified shared memory buffer and measures time to migrate it from GPU to CPU.|<ul><li>--accessAllBytes Select, whether entire resource or only one byte will be accessed on CPU (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpu|allocates a unified shared memory buffer and measures time to migrate it from CPU to GPU.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteBuffer|allocates an OpenCL buffer and measures write bandwidth. Write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|



# overlap_benchmark
Overlap Benchmark is a set of tests aimed at measuring how different commands benefit for simultaneous execution.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
KernelAndCopy|enqueues kernel and copy operation with the ability to perform both tasks on different command queues.|<ul><li>--runCopy Enqueue buffer to buffer copy during each iteration (0 or 1)</li><li>--runKernel Enqueue kernel during each iteration (0 or 1)</li><li>--twoQueues Enables using separate queues for both operations. Must be used with runCopy and runKernel (0 or 1)</li><li>--useCopyQueue Use a specialized copy queue for the copy operation. Must be used with runCopy (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
MultiProcessCompute|Creates a number of separate processes for each tile specified performing a compute workload and measures average time to complete all of them. Processes will use affinity mask to select specific sub-devices for the execution|<ul><li>--opsPerKernel Operations performed in kernel, used to steer its execution time</li><li>--processesPerTile Number of processes that will be started on each of the tiles specified</li><li>--synchronize Synchronize all processes before each iteration (0 or 1)</li><li>--tiles Tiles for execution (Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--workgroupsPerProcess Number of workgroups that each process will start</li></ul>|:heavy_check_mark:|:x:|
MultiProcessComputeSharedBuffer|Creates a number of separate processes for each tile specified performing a compute workload and measures average time to complete all of them. Processes will use affinity mask to select specific sub-devices for the execution. A single buffer for each tile is created by parent process. All processes executing on a given tile will share it via IPC calls. |<ul><li>--processesPerTile Number of processes that will be started on each of the tiles specified</li><li>--synchronize Synchronize all processes before each iteration (0 or 1)</li><li>--tiles Tiles for execution (Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--workgroupsPerProcess Number of workgroups that each process will start</li></ul>|:heavy_check_mark:|:x:|



# p2p_benchmark
P2P Benchmark is a set of tests aimed at measuring bandwidth and latency of memory transfers between peer devices.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
UsmCopyMultipleBlits|allocates two unified device memory buffers on separate devices and performs a copy between sections (or chunks) of these using a different copy engine and measures bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dstDeviceId Destination device</li><li>--size Size of the operation processed by each engine</li><li>--srcDeviceId Source device</li></ul>|:heavy_check_mark:|:x:|
UsmEUCopy|allocates two unified device memory buffers on separate devices, performs a copy between them using a compute engine, and reports bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dstDeviceId Destination device</li><li>--reuseCmdList Command list is reused between iterations (0 or 1)</li><li>--size Size of the buffer</li><li>--srcDeviceId Source device</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmImmediateCopyMultipleBlits|allocates two unified device memory buffers on separate devices and performs a copy between sections (or chunks) of these using a different copy engine with an immediate command list and measures bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dstDeviceId Destination device</li><li>--size Size of the operation processed by each engine</li><li>--srcDeviceId Source device</li></ul>|:heavy_check_mark:|:x:|



# ulls_benchmark
Ulls Benchmark is a set of tests aimed at measuring Ultra Low Latency Submission (ULLS) performance impact.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
BestSubmission|enqueues a system memory write via PIPE_CONTROL and measures when update becomes visible on the CPU.|<ul></ul>|:heavy_check_mark:|:x:|
BestWalkerSubmission|enqueues kernel, which updates system memory location and then busy-loops on CPU until the update becomes visible.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
BestWalkerSubmissionImmediate|enqueues kernel, which updates system memory location and then busy-loops on CPU until the update becomes visible. Kernel is enqueued using low-latency immediate command list, so the test is LevelZero-specific.|<ul></ul>|:heavy_check_mark:|:x:|
BestWalkerSubmissionImmediateMultiCmdlists|Append N kernels on N cmdlists, which updates system memory locations and then waits using busy-loop on CPU until the update becomes visible. Kernels are appended using immediate command lists.Amount of command lists is specified by cmdlistCount.|<ul><li>--cmdlistCount Count of command lists</li></ul>|:heavy_check_mark:|:x:|
CompletionLatency|enqueues system memory write and measures time between the moment, when update is visible on CPU and the moment, when synchronizing call returns.|<ul></ul>|:heavy_check_mark:|:x:|
CopySubmissionEvents|enqueues 4 byte copy to copy engine and return submission delta which is time between host API call and copy engine start|<ul><li>--engine Engine used for copying (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
EmptyKernel|enqueues empty kernel and measures time to launch it and wait for it on CPU, thus measuring walker spawn time.|<ul><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
EmptyKernelImmediate|enqueues empty kernel and measures time to launch it using immediate command list and wait for it on CPU, thus measuring walker spawn time.|<ul><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
KernelSwitchLatency|measures time from end of one kernel till start of next kernel|<ul><li>--barrier synchronization with barrier instead of events (0 or 1)</li><li>--flush Flush between kernels (0 or 1)</li><li>--hostVisible events are with host visible flag (0 or 1)</li><li>--kernelCount Count of kernels</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
KernelSwitchLatencyImmediate|measures time from end of one kernel till start of next kernel using immediate command lists|<ul><li>--barrier synchronization with barrier instead of events (0 or 1)</li><li>--hostVisible events are with host visible flag (0 or 1)</li><li>--kernelCount Count of kernels</li></ul>|:heavy_check_mark:|:x:|
KernelWithWork|measures time required to run a GPU kernel which assigns constant values to elements of a buffer. Each thread assigns one value.|<ul><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
KernelWithWorkImmediate|measures time required to run a GPU kernel which assigns constant values to elements of a buffer using immediate command list. Each thread assigns one value.|<ul><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
MultiProcessImmediateCmdlistCompletion|measures completion latency of AppendMemoryCopy issued from multiple processes to Immediate Command Lists.Engines to be used for submissions are selected based on the enabled bits of engineMask.Bits of the 'engineMask' are indexed from right to left. So rightmost bit represents first engine and leftmost, the last engine.'processesPerEngine' number of processes submits commands to each selected engine.If 'numberOfProcesses' is greater than 'processesPerEngine' x selected engine count, then the excess processes are assigned to selected engines one each, in a round-robin method.if selected engineCount == 1, then all processes are assigned to that engine.|<ul><li>--copySize copy size in bytes </li><li>--engineGroup engine group to be used</li><li>--engineMask bit mask for selecting engines to be used for submission</li><li>--numberOfProcesses total number of processes</li><li>--processesPerEngine number of processes submitting commands to each engine</li></ul>|:heavy_check_mark:|:x:|
MultiProcessImmediateCmdlistSubmission|measures submission latency of walker command issued from multiple processes to Immediate Command Lists.'processesPerEngine' count of processes, submit commands to each engine.If 'numberOfProcesses' is greater than 'processesPerEngine' x engine count, then the excess processes are assigned to engines one each, in a round-robin method.if engineCount == 1, then all processes are assigned to the engine.|<ul><li>--numberOfProcesses total numer of processes</li><li>--processesPerEngine number of processes submitting commands to each engine</li></ul>|:heavy_check_mark:|:x:|
MultiQueueSubmission|enqueues kernel on multiple command queues|<ul><li>--queueCount Number of command queues created</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
NewResourcesSubmissionDevice|enqueues kernel that uses a buffer placed in device memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
NewResourcesSubmissionHost|enqueues kernel that uses a buffer placed in host memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
NewResourcesWithGpuAccess|enqueues kernel that accesses an entire buffer placed in device memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
QueuePriorities|Uses queues with different priorities to meassure submission and context switch latencies|<ul><li>--highTime How long each work item is in high priority kernel</li><li>--lowTime How long each work item is in low priority kernel</li><li>--priorities Low priority command queue property is used (0 or 1)</li><li>--sleep sleep time in us after low priority kernel flushed</li><li>--wgc Workgroup count of high priority kernel</li></ul>|:x:|:heavy_check_mark:|
ResourceReassign|Enqueues stress kernel which utilizes majority of GPU's execution units, then enqueues next kernel, measuring its execution time. Shows overhead releated to GPU's resources releasing and assigning.|<ul><li>--queueCount number of different command queues to which submits after stress kernel</li></ul>|:x:|:heavy_check_mark:|
RoundTripSubmission|enqueues kernel which updates system memory location and waits for it with a synchronizing API.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedFirstCpuAccess|allocates a unified shared memory buffer and measures time to access it on CPU after creation.|<ul><li>--initialPlacement Hint for initial placement of the resource passed to the driver (Any or Host or Device)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedFirstGpuAccess|allocates a unified shared memory buffer and measures time to access it on GPU after creation.|<ul><li>--initialPlacement Hint for initial placement of the resource passed to the driver (Any or Host or Device)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WalkerCompletionLatency|enqueues a kernel writing to system memory and measures time between the moment when update is visible on CPU and the moment when synchronizing call returns|<ul><li>--useFence Use fence during submission and for further completion. (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WalkerSubmissionEvents|enqueues an empty kernel with GPU-side profiling and checks delta between queue time and start time.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteLatency|unblocks event on GPU, then waits for timestamp being written.|<ul></ul>|:heavy_check_mark:|:x:|



