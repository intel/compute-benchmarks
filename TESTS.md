# algorithm_benchmark
Algorithm Benchmark is a set of benchmarks aimed at measuring the performance of realistic worloads.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
Heat3D|A 3D heat-equation solving benchmark that overlaps IPC data transfers (nearest-neighbor halo exchange) and GPU compute kernels.Measures multi-process concurrent kernel execution and IPC memory transfer performance on a single device. Linux-only.|<ul><li>--meshLength Number of mesh points along each of the X-Y-Z directions</li><li>--subDomainX Number of sub-domains in the X-direction</li><li>--subDomainY Number of sub-domains in the Y-direction</li><li>--subDomainZ Number of sub-domains in the Z-direction</li><li>--timesteps Number of simulation timesteps</li></ul>|:heavy_check_mark:|:x:|



# api_overhead_benchmark
Api Overhead Benchmark is a set of tests aimed at measuring CPU-side execution duration of compute API calls.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
AppendLaunchKernel|measures time spent in zeCommandListAppendLaunchKernel on CPU.|<ul><li>--appendCount Number of appends to run</li><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size, pass 0 to make the driver calculate it during enqueue</li></ul>|:heavy_check_mark:|:x:|
AppendWaitOnEventsImmediate|Measures time spent to zeCommandListAppendWaitOnEvents using immediate command list.|<ul><li>--eventSignaled Event is already signaled before zeCommandListAppendWaitOnEvents call (0 or 1)</li><li>--ioq Use In order queue (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
CommandListHostSynchronize|measures CPU time spent in zeCommandListHostSynchronize. Optionally, adds an event-signalling barrierand waits for the event, before calling zeCommandListHostSynchronize|<ul><li>--UseBarrierBeforeSync Append an event-signalling-barrier before synchronization (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
CreateBuffer|measures time spent in clCreateBuffer on CPU.|<ul><li>--allocateAll Free buffers at the end of test, as opposed to freeing between iterations. Should disallow resource reuse (0 or 1)</li><li>--bufferSize Buffer size</li><li>--copyHostPtr CL_MEM_COPY_HOST_PTR flag (0 or 1)</li><li>--forceHostMemoryIntel CL_MEM_FORCE_HOST_MEMORY_INTEL flag (0 or 1)</li><li>--readOnly Read only buffer (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CreateCommandList|measures time spent in zeCommandListCreate on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li><li>--CopyOnly Create copy only cmdlist (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
CreateCommandListImmediate|measures time spent in zeCommandListCreateImmediate on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li><li>--ioq Use In order queue (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
DestroyCommandList|measures time spent in zeCommandListDestroy on CPU.|<ul><li>--CmdListCount Number of cmdlists to destroy</li></ul>|:heavy_check_mark:|:x:|
DestroyCommandListImmediate|measures time spent in zeCommandListDestroy on CPU, for immediate cmdlist.|<ul><li>--CmdListCount Number of immediate cmdlists to create</li><li>--ioq Use In order queue (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
DriverGet|measures time spent in driver get call on CPU.|<ul><li>--getDriverCount Whether to measure driver count or driver get (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
DriverGetApiVersion|measures time spent in zeDriverGetApiVersion call on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
DriverGetProperties|measures time spent in zeDriverGetProperties call on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
EnqueueNdrNullLws|measures time spent in clEnqueueNDRangeKernel on CPU. Null LWS is provided, which causes driver to calculate it|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--gws Global work size</li><li>--ooq Use out of order queue (0 or 1)</li><li>--profiling Creating a profiling queue (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
EnqueueNdrTime|measures time spent in clEnqueueNDRangeKernel on CPU.|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--ooq Use out of order queue (0 or 1)</li><li>--profiling Creating a profiling queue (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size</li></ul>|:x:|:heavy_check_mark:|
EventCreation|measures time spent to create event|<ul><li>--eventCount Number of events to create</li><li>--hostVisible Event will set host visible flag (0 or 1)</li><li>--signal Type of signal scope (subdevice or device or host or none)</li><li>--useProfiling Event will use profiling (0 or 1)</li><li>--wait Type of wait scope (subdevice or device or host or none)</li></ul>|:heavy_check_mark:|:x:|
EventQueryStatus|Measures time spent to query event status|<ul><li>--eventSignaled Event will be set as signaled (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecImmediate|measures time spent in appending launch kernel for immediate command list on CPU.|<ul><li>--BarrierSynchro Uses barrier synchronization instead of waiting for event from last kernel (0 or 1)</li><li>--CallsCount amount of calls that is being meassured</li><li>--EventSync If true, use events to synchronize with host. If false, use zeCommandListHostSynchronize (0 or 1)</li><li>--KernelExecTime How long a single kernel executes, in us</li><li>--MeasureCompletion Measures time taken to complete the submission (default is to measure only Immediate call) (0 or 1)</li><li>--Profiling Pass a profiling ze_event_t to the API call (0 or 1)</li><li>--ioq Use In order queue (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecImmediateCopyQueue|measures time spent in appending memory copy for immediate command list on CPU with Copy Queue.|<ul><li>--IsCopyOnly If true, Copy Engine is selected. If false, Compute Engine is selected (0 or 1)</li><li>--MeasureCompletionTime Measures time taken to complete the submission (default is to measure only Immediate call) (0 or 1)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--ioq Use In order queue (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecImmediateMultiKernel|measures time spent in executing multiple instances of two different kernels with immediate command list on CPU|<ul><li>--AfterBarrierCnt Adds certain number of kernels after Barrier, Default is 2</li><li>--Barrier Add a Barrier after certain number of Kernel launches, number of kernels before barrier is controlled by numKernelsBeforeBarrier (0 or 1)</li><li>--BeforeBarrierCnt Adds certain number of kernels prior to Barrier, Default is 2</li><li>--CallsCount amount of calls that is being measured</li><li>--ExecTime Approximately how long a single kernel executes, in us</li><li>--ioq Use In order queue (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandList|measures time spent in zeCommandQueueExecuteCommandLists on CPU.|<ul><li>--UseFence Pass a non-null ze_fence_handle_t to the API call (0 or 1)</li><li>--measureCompletionTime Measures time taken to complete the submission (default is to measure only Execute call) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListForCopyEngine|measures CPU time spent in zeCommandQueueExecuteCommandLists for copy-only path|<ul><li>--UseFence Pass a non-null ze_fence_handle_t to the API call (0 or 1)</li><li>--measureCompletionTime Measures time taken to complete the submission (default is to measure only Execute call) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceCreate|measures time spent in zeFenceCreate on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceDestroy|measures time spent in zeFenceDestroy on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithFenceUsage|measures time spent in zeCommandQueueExecuteCommandLists and zeFenceSynchronize on CPU when fences are used.|<ul></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithIndirectAccess|measures time spent in zeCommandQueueExecuteCommandLists on CPU when indirect allocations are accessed.|<ul><li>--AllocateMemory If set then prior to measurement new allocation is done and made resident. (0 or 1)</li><li>--AmountOfIndirectAllocations Amount of indirect allocations that are present in system</li></ul>|:heavy_check_mark:|:x:|
ExecuteCommandListWithIndirectArguments|measures time spent in zeCommandQueueExecuteCommandLists on CPU when indirect allocations are used.|<ul><li>--AmountOfIndirectAllocations Amount of indirect allocations that are present in system</li><li>--placement Placement of the indirect allocations (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
ExecuteRegularCommandListWithImmediate|measures time spent in zeCommandListImmediateAppendCommandListsExp on CPU.|<ul><li>--ctrBasedEvents use counter based events for in order (0 or 1)</li><li>--event Pass output event to the enqueue call  (0 or 1)</li><li>--ioq use in order queue/command list (0 or 1)</li><li>--measureCompletionTime Measures time taken to complete the submission  (0 or 1)</li><li>--useProfiling Event will use profiling  (0 or 1)</li><li>--waitEvent use preceeding operation to wait on (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
FlushTime|measures time spent in clEnqueueNDRangeKernel on CPU.|<ul><li>--event Pass output event to the enqueue call (0 or 1)</li><li>--flushCount Count of flushes to measure</li><li>--ooq Use out of order queue (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size, pass 0 to make the driver calculate it during enqueue</li></ul>|:x:|:heavy_check_mark:|
GetMemoryProperties|measures time spent in zeMemGetAllocProperties on CPU when driver is queried for memory properties.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li></ul>|:heavy_check_mark:|:x:|
GetMemoryPropertiesWithModifiedAllocations|measures time spent in zeMemGetAllocProperties on CPU, when allocations are modified between each iteration.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li></ul>|:heavy_check_mark:|:x:|
GetMemoryPropertiesWithOffsetedPointer|measures time spent in zeMemGetAllocProperties on CPU when the pointer passed is an offset from the base address.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li></ul>|:heavy_check_mark:|:x:|
KernelSetArgumentValueImmediate|measures time spent in zeKernelSetArgumentValue for immediate arguments on CPU.|<ul><li>--argSize Kernel argument size in bytes</li><li>--differentValues Use different values for arguments each iteration (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
LifecycleCommandList|measures time spent in zeCommandListCreate + Close + Execute on CPU.|<ul><li>--CmdListCount Number of cmdlists to create</li><li>--CopyOnly Create copy only cmdlist (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
MemGetIpcHandle|measures time spent in zeMemGetIpcHandle on CPU.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
MemOpenIpcHandle|measures time spent in zeMemOpenIpcHandle on CPU.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
MemPutIpcHandle|measures time spent in zeMemPutIpcHandle on CPU.|<ul><li>--AmountOfUsmAllocations Amount of USM allocations that are present in system</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
ModuleCreateSpv|measures time spent in zeModuleCreate for .spv kernel on CPU.|<ul><li>--kernelName Path to Kernel .spv file</li></ul>|:heavy_check_mark:|:x:|
MultiArgumentKernelTime|measures time spent in clEnqueueNDRangeKernel on CPU for kernels with multiple arguments.|<ul><li>--argumentCount argument count in a kernel, supported values 1,4,8,16,32,64</li><li>--count how many kernel launches are measured</li><li>--exec measure execute as well (0 or 1)</li><li>--groupCount total amount of work groups</li><li>--ids control whether kernel uses global ids (0 or 1)</li><li>--lws local work size</li><li>--measureSetArg control whether setKernelArgSvmPointer is measured or not (0 or 1)</li><li>--newApi utilize zeCommandListAppendLaunchKernelWithArguments for L0 submissions (0 or 1)</li><li>--reverseOrder set kernel arguments in reverse order (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
PhysicalMemCreate|measures time spent in zePhysicalMemCreate on CPU.|<ul><li>--reserveSize Size in bytes to be reserved</li></ul>|:heavy_check_mark:|:x:|
PhysicalMemDestroy|measures time spent in zePhysicalMemDestroy on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
ResetCommandList|measures time spent in zeCommandListReset on CPU.|<ul><li>--CopyOnly Create copy only cmdlist (0 or 1)</li><li>--size Size of the buffer</li><li>--sourcePlacement Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
SetKernelArgSvmPointer|measures time spent in clSetKernelArgSVMPointer on CPU.|<ul><li>--allocationSize Size of svm allocations, in bytes</li><li>--allocationsCount Number of allocations</li><li>--reallocate Allocations will be freed and allocated again between setKernelArgs (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
SetKernelGroupSize|measures time spent in zeKernelSetGroupSize on CPU.|<ul><li>--asymmetricLocalWorkSize Use asymmetric local workSize (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
SubmitKernel|measures time spent in submitting a kernel to a SYCL (or SYCL-like) queue on CPU.|<ul><li>--Ioq Create the queue with the in_order property (0 or 1)</li><li>--KernelExecTime Approximately how long a single kernel executes, in us</li><li>--MeasureCompletion Measures time taken to complete the submission (default is to measure only submit calls) (0 or 1)</li><li>--NumKernels Number of kernels to submit to the queue</li><li>--Profiling Create the queue with the enable_profiling property (0 or 1)</li><li>--UseEvents Use events when enqueuing kernels. When false, SYCL will use eventless enqueue functions. (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmMemoryAllocation|measures time spent in USM memory allocation APIs.|<ul><li>--measureMode Specifies which APIs to measure (Allocate or Free or Both)</li><li>--size Size to allocate</li><li>--type Type of memory being allocated (Device or Host or Shared)</li></ul>|:heavy_check_mark:|:x:|
VirtualMemFree|measures time spent in zeVirtualMemFree on CPU.|<ul><li>--freeSize Size in bytes to be freed</li></ul>|:heavy_check_mark:|:x:|
VirtualMemGetAccessAttrib|measures time spent in zeVirtualMemGetAccessAttribute on CPU.|<ul><li>--size Size in bytes to get the access attribute</li></ul>|:heavy_check_mark:|:x:|
VirtualMemMap|measures time spent in zeVirtualMemMap on CPU.|<ul><li>--accessType Access type. Either 'ReadWrite' or 'ReadOnly'</li><li>--reserveSize Size in bytes to be reserved</li><li>--useOffset Use offset to map into physical memory (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
VirtualMemQueryPageSize|measures time spent in zeVirtualMemQueryPageSize on CPU.|<ul></ul>|:heavy_check_mark:|:x:|
VirtualMemReserve|measures time spent in zeVirtualMemReserve on CPU.|<ul><li>--reserveSize Size in bytes to be reserved</li><li>--useNull Flag to decide whether Null to be used for start of region (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
VirtualMemSetAccessAttrib|measures time spent in zeVirtualMemSetAccessAttribute on CPU.|<ul><li>--accessType Access type to set. Either 'ReadWrite', 'ReadOnly' or 'None'</li><li>--size Size in bytes to set the access attribute</li></ul>|:heavy_check_mark:|:x:|
VirtualMemUnMap|measures time spent in zeVirtualMemUnMap on CPU.|<ul><li>--reserveSize Size in bytes to be unmapped</li></ul>|:heavy_check_mark:|:x:|



# atomic_benchmark
Atomic Benchmark is a set of tests aimed at measuring performance of atomic operations inside kernels.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
OneAtomic|enqueues kernel performing an atomic operation on a single address|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneAtomicExplicit|enqueues kernel performing an atomic operation on a single address using OpenCL 2.0 Atomics with explicit memory order and scope|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneLocalAtomic|enqueues kernel performing an atomic operation on a single location placed in SLM|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
OneLocalAtomicExplicit|enqueues kernel performing an atomic operation on a single location placed in SLM using OpenCL 2.0 Atomics with explicit memory order and scope|<ul><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
SeparateAtomics|enqueues kernel performing an atomic operation on different addresses|<ul><li>--atomicsPerCacheline Number of used addresses occupying a single cacheline (this causes operations to be serialized)</li><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
SeparateAtomicsExplicit|enqueues kernel performing an atomic operation on different addresses|<ul><li>--atomicsPerCacheline Number of used addresses occupying a single cacheline (this causes operations to be serialized)</li><li>--op Atomic operation to perform (Add or Sub or Xchg or CmpXchg or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--order Memory order of an atomic operation (relaxed or acquire or release or acq_rel or seq_cst)</li><li>--scope Memory scope of an atomic operation (Workgroup or Device)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|



# emu_benchmark
Emulation Benchmark is a set of tests aimed at measuring performance of emulated math operations performed in kernels.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
Int64Div|enqueues kernel performing an int64 division emulation|<ul><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|



# eu_benchmark
EU Benchmark is a set of tests aimed at measuring performance of calculations performed in kernels.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
DoMathOperation|enqueues kernel performing a math operation|<ul><li>--mixGrf Run kernels with mixed grf modes (0 or 1)</li><li>--op Math operation to perform (Add or Sub or Div or Modulo or Inc or Dec or Min or Max or And or Or or Xor)</li><li>--type Data type of the atomic. Keep in mind not all operations are supported for floating points (Int32 or Int64 or Float)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgc Work group count</li><li>--wgs Work group size</li></ul>|:x:|:heavy_check_mark:|
ReadAfterAtomicWrite|enqueues kernel, which writes to global memory using atomic and then reads non atomically|<ul><li>--atomic If true, write to global memory will be atomic. (0 or 1)</li><li>--shuffleRead If true, each thread will write and read different memory cell. Otherwise it will be the same one. (0 or 1)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--wgs Workgroup size</li></ul>|:x:|:heavy_check_mark:|



# gpu_cmds_benchmark
Gpu Commands Benchmark is a set of tests aimed at measuring GPU-side execution duration of various commands.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
BarrierBetweenKernels|measures time required to run a barrier command between 2 kernels, including potential cache flush commands|<ul><li>--barrierCount total amount of measured barriers</li><li>--bytes bytes to flush from L3</li><li>--memoryType memory type cached in L3 (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--onlyReads only reads cached in L3</li><li>--remoteAccess access cached from remote tile</li></ul>|:heavy_check_mark:|:x:|
CopyWithEvent|measures time required to run a copy kernel with various event configurations.|<ul><li>--devWaitEvent Use ZE_EVENT_SCOPE_FLAG_DEVICE for ze_event_desc_t::wait (0 or 1)</li><li>--hostSignalEvent Use ZE_EVENT_POOL_HOST_VISIBLE for ze_event_pool_desc_t::flags, and use ZE_EVENT_SCOPE_FLAG_HOST for ze_event_desc_t::signal (0 or 1)</li><li>--measuredCmds Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--timestampEvent Use ZE_EVENT_POOL_FLAG_KERNEL_TIMESTAMP for ze_event_pool_desc_t::flags (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
EmptyKernel|measures time required to run an empty kernel on GPU.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
EventCtxtSwitchLatency|measures context switching latency time required to switch between various engine types|<ul><li>--firstEngine first engine to measure context switch latency (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--secondEngine second engine to measure context switch latency (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li></ul>|:heavy_check_mark:|:x:|
KernelWithEvent|measures time required to run an empty kernel with various event configurations.|<ul><li>--devWaitEvent Use ZE_EVENT_SCOPE_FLAG_DEVICE for ze_event_desc_t::wait (0 or 1)</li><li>--hostSignalEvent Use ZE_EVENT_POOL_HOST_VISIBLE for ze_event_pool_desc_t::flags, and use ZE_EVENT_SCOPE_FLAG_HOST for ze_event_desc_t::signal (0 or 1)</li><li>--measuredCmds Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--timestampEvent Use ZE_EVENT_POOL_FLAG_KERNEL_TIMESTAMP for ze_event_pool_desc_t::flags (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
KernelWithWork|measures time required to run a GPU kernel which assigns values to elements of a buffer.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
LastEventLatency|Measures different ways to retrieve event from last submission.|<ul><li>--signalOnBarrier Place signal event on barrier instead of launched kernel. (0 or 1)</li><li>--useSameCmdList Synchronize on barrier using same cmd list. (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventCold|measures time required to service a signalled semaphore, that has never been waited for.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventFromWalker|measures time required to service a signalled semaphore coming from Walker command|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WaitOnEventHot|measures time required to service a signalled semaphore, that was previously used|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|
WriteTimestamp|measures time required to write a timestamp on GPU.|<ul><li>--measuredCommands Number of commands being measured. Result is later divided by this number, to achieve time of a single command</li></ul>|:heavy_check_mark:|:x:|



# graph_api_benchmark
Graph Api Overhead Benchmark is a set of tests aimed at measuring CPU-side execution duration of SYCL Graphs API calls.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
MutateGraph|The benchmark quantifies the benefits of using Mutable Command List for graphs compared to recreating the graphs for scratch every time|<ul><li>--canUpdate If true, the benchmark graph changes are done using L0 mutable command list features, otherwise they are created every time from scratch. (0 or 1)</li><li>--changeRate Rate at which kernels should be changed (for example, 5 means that every 5th kernel in the graph will be changed)</li><li>--mutateIsa If true, the benchmark will mutate kernel ISA. (0 or 1)</li><li>--numKernels Number of kernels to use in a graph</li><li>--operationType Determines operation type that should be measured: initialization, modification, execution or creation (Initialize or Mutate or Execute or Create)</li><li>--useInOrder If true, lists are created with ZE_COMMAND_LIST_FLAG_IN_ORDER flag, otherwise they are created without it (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
SinKernelGraph|Benchmark running memory copy and kernel runs, with graphs and without graphs|<ul><li>--immediateAppendCmdList Use zeCommandListImmediateAppendCommandListsExp to submit graph (only valid for L0) (0 or 1)</li><li>--numKernels Number of kernel invocations</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li><li>--withGraphs Runs with or without graphs (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
SubmitGraph|measures time spent in submitting a graph to a SYCL (or SYCL-like) queue on CPU.|<ul><li>--InOrderQueue Create the queue with the in_order property (0 or 1)</li><li>--KernelExecutionTime Approximately how long a single kernel executes, in us</li><li>--MeasureCompletionTime Measures time taken to complete the submission (default is to measure only submit calls) (0 or 1)</li><li>--NumKernels Number of kernels to submit to the queue</li><li>--Profiling Create the queue with the enable_profiling property (0 or 1)</li><li>--UseEvents Do not create and track events for synchronization (0 or 1)</li><li>--UseExplicit Using Explicit Graph Creation mode vs. Record and Replay (0 or 1)</li><li>--UseHostTasks Submit SYCL host task after kernel enqueue (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|



# memory_benchmark
Memory Benchmark is a set of tests aimed at measuring bandwidth of memory transfers.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
CopyBuffer|allocates two OpenCL buffers and measures copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--compressedDestination Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--compressedSource Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffers (Zeros or Random)</li><li>--size Size of the buffers</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CopyBufferRect|allocates two OpenCL buffers and measures rectangle copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--dstCompressed Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li><li>--srcCompressed Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
CopyBufferToImage|allocates buffer and image and measures copy bandwidth between them using immediate command list for Level Zero and command queue for OpenCL.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--region Size of the destination image region</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
CopyEntireImage|allocates two image objects and measures copy bandwidth between them. Images will be placed in device memory, if it's available.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the image</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
CopyImageRegion|allocates two image objects and measures region copy bandwidth between them using immediate command list for Level Zero and command queue for OpenCL.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the image</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
CopyImageToBuffer|allocates image and buffer and measures copy bandwidth between them using immediate command list for Level Zero and command queue for OpenCL.|<ul><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--region Size of the source image region</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
FillBuffer|allocates an OpenCL buffer and measures fill bandwidth. Buffer will be placed in device memory, if it's available.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
FullRemoteAccessMemory|Uses stream memory in a fashion described by 'type' to measure bandwidth of full remote memory access.|<ul><li>--blockAccess Block access (1) or scatter access (0) (0 or 1)</li><li>--elementSize Size of the single element to read in bytes (1, 2, 4, 8)</li><li>--size Size of the memory to stream. Must be divisible by element size and a power of 2</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--workItems Number of work items equal to SIMD size * used hwthreads. Must be a power of 2</li></ul>|:x:|:heavy_check_mark:|
FullRemoteAccessMemoryXeCoresDistributed|Uses stream memory in a fashion described by 'type' to measure bandwidth of full remote memory accesswhen hwthreads are distributed between XeCores.|<ul><li>--blockAccess Block access (1) or scatter access (0) (0 or 1)</li><li>--elementSize Size of the single element to read in bytes (1, 2, 4, 8)</li><li>--size Size of the memory to stream. Must be a power of 2</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--workItems Number of work items equal to SIMD size * used hwthreads</li></ul>|:x:|:heavy_check_mark:|
MapBuffer|allocates an OpenCL buffer and measures map bandwidth. Mapping operation means memory transfer from GPU to CPU or a no-op, depending on map flags.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--mapFlags OpenCL map flags passed during memory mapping (Read or Write or WriteInvalidate)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
NonUsmCopy|Measures time for non usm transfers and further potential host updates, updates/reallocates memory on the host between copies, uses immediate command lists and copy offload|<ul><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--reallocate reallocate buffers before each copy (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--updateOnHost memory is updated on the host between copies (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
QueueInOrderMemcpy|measures time on CPU spent for multiple in order memcpy.|<ul><li>--IsCopyOnly If true, Copy Engine is selected. If false, Compute Engine is selected (0 or 1)</li><li>--count Number of memcpy operations</li><li>--destinationPlacement Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--size Size of memory allocation</li><li>--sourcePlacement Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
RandomAccessMemory|Measures device-memory random access bandwidth for different allocation sizes, alignments and access modes.The benchmark uses 10 million accesses to memory.|<ul><li>--accessMode Access mode to be used('Read', 'Write', 'ReadWrite')</li><li>--alignment Alignment request for the allocated memory</li><li>--allocationSize Size of device memory to be allocated.(Maximum supported is 16GB)</li><li>--randomAccessRange Percentage of allocation size to be used for random access</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ReadBuffer|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--reuse How hostptr allocation can be reused due to previous operations (Aligned4KB or Misaligned or Usm or Map)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBufferMisaligned|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU. Destination pointer passed by the application will be misaligned by the specified amount of bytes.|<ul><li>--misalignment Number of bytes by which misaligned the destination pointer will be misaligned</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBufferRect|allocates an OpenCL buffer and measures rectangle read bandwidth. Rectangle read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|
ReadDeviceMemBuffer|allocates two OpenCL buffers and measures source buffer read bandwidth. Source buffer resides in device memory.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|
ReadImage|Measures time spent during Read Image calls |<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--ptrPlacement memory placement of host_ptr passed to ReadImage call (Aligned4KB or Misaligned or Usm or Map)</li><li>--size Size of the image</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
RemoteAccessMemory|Uses stream memory in a fashion described by 'type' to measure bandwidth with differentpercentages of remote memory access. Triad means two buffers are read and one is written to.In read and write memory is only read or written to.|<ul><li>--remoteFraction Fraction of remote memory access. 1 / n</li><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--workItemSize Number of work items group together for remote check</li></ul>|:x:|:heavy_check_mark:|
RemoteAccessMemoryMaxSaturation|Uses stream memory write to measure max data bus saturation with different percentages of remote memory access|<ul><li>--remoteFraction Fraction of remote memory access. 1 / n</li><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--workItemSize Number of work items group together for remote check</li><li>--writesPerWorkgroup Number of work items per workgroup that access memory</li></ul>|:x:|:heavy_check_mark:|
SLM_DataAccessLatency|generates SLM local memory transactions inside thread group to measure latency between reads (uses Intel only private intel_get_cycle_counter() )|<ul><li>--direction write or read mode (0 or 1)</li><li>--occupancyDiv H/W load divider by 8, 4, 2, full occupancy</li><li>--size SLM Size</li></ul>|:x:|:heavy_check_mark:|
SlmSwitchLatency|Enqueues 2 kernels with different SLM size. Measures switch time between these kernels.|<ul><li>--firstSlmSize Size of the shared local memory per thread group. First kernel.</li><li>--secondSlmSize Size of the shared local memory per thread group. Second kernel.</li><li>--wgs Size of the work group.</li></ul>|:heavy_check_mark:|:x:|
StreamAfterTransfer|Goal of this test is to measure how stream kernels perform right after host to device transfer populating the data. Test does clean caches, then emits transfers and then follows with stream kernel and measures GPU execution time of it.|<ul><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li></ul>|:x:|:heavy_check_mark:|
StreamMemory|Streams memory inside of kernel in a fashion described by 'type'. Copy means one memory location is read from and the second one is written to. Triad means two buffers are read and one is written to. In read and write memory is only read or written to.|<ul><li>--contents Buffer contents zeros/random (Zeros or Random)</li><li>--lws local work size</li><li>--memoryPlacement Memory type used for stream (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--multiplier multiplies id used for accessing the resources to simulate partials</li><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--vectorSize size of uint vector type 1/2/4/8/16</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
StreamMemoryImmediate|Streams memory inside of kernel in a fashion described by 'type' using immediate command list. Copy means one memory location is read from and the second one is written to. Triad means two buffers are read and one is written to. In read and write memory is only read or written to.|<ul><li>--size Size of the memory to stream. Must be divisible by datatype size.</li><li>--type Memory streaming type (Read or Write or Scale or Triad)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UnmapBuffer|allocates an OpenCL buffer and measures unmap bandwidth. Unmapping operation meansmemory transfer from CPU to GPU or a no-op, depending on map flags.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--mapFlags OpenCL map flags passed during memory mapping (Read or Write or WriteInvalidate)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmConcurrentCopy|allocates four unified shared memory buffers, 2 in device memory and 2 in host memory. Measures concurrent copy bandwidth between them.|<ul><li>--d2hEngine Engine used for device to host copy (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li><li>--h2dEngine Engine used for host to device copy (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li><li>--size Size of the buffer</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopy|allocates two unified shared memory buffers and measures copy bandwidth between them.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--reuseCmdList Command list is reused between iterations (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyConcurrentMultipleBlits|Measures Copy bandwidth while performing concurrent copies between host and device using different copy engines. Engines for Host to Device copies could be selected using h2dBlitters. Engines for Device to Host copies could be selected using d2hBlitters.|<ul><li>--d2hBlitters A bit mask for selecting copy engines to be used for device to host copy</li><li>--h2dBlitters A bit mask for selecting copy engines to be used for host to device copy</li><li>--size Size of the copy to be done for each copy engine</li></ul>|:heavy_check_mark:|:x:|
UsmCopyImmediate|allocates two unified shared memory buffers and measures copy bandwidth between them using immediate command list.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyMultipleBlits|allocates two unified shared memory buffers, divides them into chunks, copies each chunk using a different copy engine and measures bandwidth. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--size Size of the operation processed by each engine</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyRegion|allocates two unified shared memory buffers and measures region copy bandwidth between them using immediate command list.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--origin Origin of the region</li><li>--region Size of the region</li><li>--size Size of the buffer</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyStagingBuffers|Measures copy time from device/host to host/device. Host memory is non-USM allocation.Copy is done through staging USM buffers. Non-USM host ptr is never passed to L0 API, only through staging buffers.|<ul><li>--chunks How much memory chunks should the buffer be splitted into</li><li>--dst Memory placement of destination (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffer</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmFill|allocates a unified memory buffer and measures fill bandwidth|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFillImmediate|allocates a unified memory buffer and measures fill bandwidth using immediate command list|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmFillMultipleBlits|allocates a unified shared memory buffer, divides it into chunks, copies each chunk using a different copy engine and measures bandwidth. Refer to UsmCopyMultipleBlits for more details.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--memory Placement of buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--patternContents Select contents of the fill pattern (Zeros or Random)</li><li>--patternSize Size of the fill pattern</li><li>--size Size of the operation processed by each engine</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFillSpecificPattern|allocates a unified memory buffer and measures fill bandwidth. Allow specifying arbitrary pattern.|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--pattern The fill pattern represented hexadecimally, e.g. 0x91ABCD1254</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmImmediateCopyMultipleBlits|allocates two unified shared memory buffers, divides them into chunks, copies each chunk using a different copy engine with an immediate command list and  measures bandwidth. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dst Placement of the destination buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--size Size of the operation processed by each engine</li><li>--src Placement of the source buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li></ul>|:heavy_check_mark:|:x:|
UsmMemset|allocates a unified memory buffer and measures memset bandwidth|<ul><li>--contents Contents of the buffer (Zeros or Random)</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of the buffer (Device or Host or Shared or non-USM-mapped or non-USMmisaligned or non-USM4KBAligned or non-USM2MBAligned or non-USMmisaligned-imported or non-USM4KBAligned-imported or non-USM2MBAligned-imported or non-USM)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmSharedMigrateCpu|allocates a unified shared memory buffer and measures bandwidth for kernel that must migrate resource from GPU to CPU|<ul><li>--accessAllBytes Select, whether entire resource or only one byte will be accessed on CPU (0 or 1)</li><li>--preferredLocation Apply memadvise with preferred device location (system, device, none) (System or Device or None)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpu|allocates a unified shared memory buffer and measures bandwidth for kernel that must migrate resource from CPU to GPU|<ul><li>--preferredLocation Apply memadvise with preferred device location (system, device, none) (System or Device or None)</li><li>--prefetch Explicitly migrate shared allocation to device associated with command queue (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpuForFill|allocates a unified shared memory buffer and measures bandwidth for memory fill operation that must migrate resource from CPU to GPU|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--preferredLocation Apply memadvise with preferred device location (system, device, none) (System or Device or None)</li><li>--prefetch Explicitly migrate shared allocation to device associated with command queue (0 or 1)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteBuffer|allocates an OpenCL buffer and measures write bandwidth. Write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--contents Contents of the buffer (Zeros or Random)</li><li>--reuse How hostptr allocation can be reused due to previous operations (Aligned4KB or Misaligned or Usm or Map)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
WriteBufferRect|allocates an OpenCL buffer and measures rectangle write bandwidth. Rectangle write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--inOrderQueue If set use IOQ, otherwise OOQ. Applicable only for OCL. (0 or 1)</li><li>--origin Origin of the rectangle</li><li>--rPitch Row pitch of the rectangle</li><li>--region Size of the rectangle</li><li>--sPitch Silice pitch of the rectangle</li><li>--size Size of the buffer</li></ul>|:x:|:heavy_check_mark:|
WriteImage|Measures time spent during Write Image calls |<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--ptrPlacement memory placement of host_ptr passed to WriteImage call (Aligned4KB or Misaligned or Usm or Map)</li><li>--size Size of the image</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|



# miscellaneous_benchmark
Miscellaneous Benchmark is a set of tests measuring different simple compute scenarios.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
IoqKernelSwitchLatency|measures time from end of one kernel till start of next kernel for in order queue|<ul><li>--kernelCount Count of kernels</li><li>--useEvents Use events to synchronize between kernels (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
KernelWithWork|measures time required to run a GPU kernel which assigns constant values to elements of a buffer. Each thread assigns one value. Benchmark checks the impact of kernel split.|<ul><li>--split How many times kernel is split</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:x:|:heavy_check_mark:|
Reduction|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction2|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction3|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction4|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
Reduction5|Performs a reduction operation on a buffer. Each thread performs atomic_add on one shared memory location.|<ul><li>--numberOfElements Number of elements that will be reduced</li></ul>|:x:|:heavy_check_mark:|
VectorSum|Performs vector addition|<ul><li>--numberOfElementsX Number of elements in X dimension</li><li>--numberOfElementsY Number of elements in Y dimension</li><li>--numberOfElementsZ Number of elements in Z dimension</li></ul>|:x:|:heavy_check_mark:|



# multiprocess_benchmark
Multiprocess Benchmark is a set of tests aimed at measuring how different commands benefit for simultaneous execution.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
KernelAndCopy|enqueues kernel and copy operation with the ability to perform both tasks on different command queues.|<ul><li>--runCopy Enqueue buffer to buffer copy during each iteration (0 or 1)</li><li>--runKernel Enqueue kernel during each iteration (0 or 1)</li><li>--twoQueues Enables using separate queues for both operations. Must be used with runCopy and runKernel (0 or 1)</li><li>--useCopyQueue Use a specialized copy queue for the copy operation. Must be used with runCopy (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
MultiProcessCompute|Creates a number of separate processes for each tile specified performing a compute workload and measures average time to complete all of them. Processes will use affinity mask to select specific sub-devices for the execution|<ul><li>--opsPerKernel Operations performed in kernel, used to steer its execution time</li><li>--processesPerTile Number of processes that will be started on each of the tiles specified</li><li>--synchronize Synchronize all processes before each iteration (0 or 1)</li><li>--tiles Tiles for execution (Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--workgroupsPerProcess Number of workgroups that each process will start</li></ul>|:heavy_check_mark:|:x:|
MultiProcessComputeSharedBuffer|Creates a number of separate processes for each tile specified performing a compute workload and measures average time to complete all of them. Processes will use affinity mask to select specific sub-devices for the execution. A single buffer for each tile is created by parent process. All processes executing on a given tile will share it via IPC calls. |<ul><li>--processesPerTile Number of processes that will be started on each of the tiles specified</li><li>--synchronize Synchronize all processes before each iteration (0 or 1)</li><li>--tiles Tiles for execution (Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--workgroupsPerProcess Number of workgroups that each process will start</li></ul>|:heavy_check_mark:|:x:|
MultiProcessImmediateCmdlistCompletion|measures completion latency of AppendMemoryCopy issued from multiple processes to Immediate Command Lists.Engines to be used for submissions are selected based on the enabled bits of engineMask.Bits of the 'engineMask' are indexed from right to left. So rightmost bit represents first engine and leftmost, the last engine.'processesPerEngine' number of processes submits commands to each selected engine.If 'numberOfProcesses' is greater than 'processesPerEngine' x selected engine count, then the excess processes are assigned to selected engines one each, in a round-robin method.if selected engineCount == 1, then all processes are assigned to that engine.|<ul><li>--copySize copy size in bytes </li><li>--engineGroup engine group to be used</li><li>--engineMask bit mask for selecting engines to be used for submission</li><li>--numberOfProcesses total number of processes</li><li>--processesPerEngine number of processes submitting commands to each engine</li></ul>|:heavy_check_mark:|:x:|
MultiProcessImmediateCmdlistSubmission|measures submission latency of walker command issued from multiple processes to Immediate Command Lists.'processesPerEngine' count of processes, submit commands to each engine.If 'numberOfProcesses' is greater than 'processesPerEngine' x engine count, then the excess processes are assigned to engines one each, in a round-robin method.if engineCount == 1, then all processes are assigned to the engine.|<ul><li>--numberOfProcesses total numer of processes</li><li>--processesPerEngine number of processes submitting commands to each engine</li></ul>|:heavy_check_mark:|:x:|
MultiProcessInit|Measures the initialization overhead in a multi-process application.For Level Zero we only measure the first invocation of zeInit() per process execution.|<ul><li>--initFlag Initialization flag. For Level Zero: 0 - default, 1 - ZE_INIT_FLAG_GPU_ONLY, 2 - ZE_INIT_FLAG_VPU_ONLY</li><li>--numberOfProcesses Total number of processes</li></ul>|:heavy_check_mark:|:x:|



# multithread_benchmark
Multithread Benchmark is a set of tests aimed at measuring how different commands benefit from multithreaded execution.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
ImmediateCommandListCompletion|measures completion latency of AppendMemoryCopy issued from multiple threads to Immediate Command Lists.Engines to be used for submissions are selected based on the enabled bits of engineMask.'threadsPerEngine' number of threads submits commands to each selected engine.If 'numberOfThreads' is greater than 'threadsPerEngine' x selected engine count, then the excess threads are assigned to selected engines one each, in a round-robin method.if selected engineCount == 1, then all threads are assigned to that engine.|<ul><li>--copySize copy size in bytes </li><li>--engineGroup engine group to be used</li><li>--engineMask bit mask for selecting engines to be used for submission</li><li>--numberOfThreads total number of threads</li><li>--threadsPerEngine number of threads submitting commands to each engine</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
ImmediateCommandListSubmission|measures submission latency of AppendLaunchKernel issued from multiple threads to Immediate Command Lists.'threadsPerEngine' count of threads submit commands to each engine.If 'numberOfThreads' is greater than 'threadsPerEngine' x engine count, then the excess threads are assigned to engines one each, in a round-robin method.if engineCount == 1, then all threads are assigned to the engine.|<ul><li>--numberOfThreads total number of threads</li><li>--threadsPerEngine number of threads submitting commands to each engine</li></ul>|:heavy_check_mark:|:x:|
SvmCopy|enqueues multiple svm copies on multiple threads concurrently.|<ul><li>--numberOfThreads Number of threads that will run concurrently</li></ul>|:heavy_check_mark:|:heavy_check_mark:|



# multitile_memory_benchmark
Multi-tile Memory Benchmark is a set of tests aimed at measuring bandwidth of memory transfers performed on a multi-tile device.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
CopyBuffer|allocates two OpenCL buffers and measures copy bandwidth between them. Buffers will be placed in device memory, if it's available.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--dstCompressed Select if the destination buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--srcCompressed Select if the source buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
FillBuffer|allocates an OpenCL buffer and measures fill bandwidth. Buffer will be placed in device memory, if it's available.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--patternSize Size of the fill pattern</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
ReadBuffer|allocates an OpenCL buffer and measures read bandwidth. Read operation means transfer from GPU to CPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
UsmBidirectionalCopy|allocates two unified device memory buffers, each on a different tile, and measures copy bandwidth between. Test measures copies on two directions, which can be controlled with the -write parameter: with -write=1, each tile performs a write operation. For instance: queue is placed in tile 0, source is buffer in tile 0, and destination is in tile 1. Similarly for tile 1, queue is placed in tile 1, source in tile 1, and destination in tile 0. With -write=0, the destination and source are flipped: queue is placed in tile 0, source is buffer in tile 1, and destination is in tile 0, while for tile 1, queue is placed in tile 1, source in tile 0, and destination in tile 1.|<ul><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--size Size of the buffers</li><li>--write Which operation is used, whether write or read (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopy|allocates two unified shared memory buffers and measures copy bandwidth between them using a builtin function.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmCopyImmediate|allocates two unified shared memory buffers and measures copy bandwidth between them using a builtin function appended to an immediate list.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li><li>--withCopyOffload Enable driver copy offload (only valid for L0) (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmCopyKernel|allocates two unified shared memory buffers and measures copy bandwidth between them using a custom kernel.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--dst Placement of memory for the destination buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffers</li><li>--src Placement of memory for the source buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmFill|allocates a unified shared memory buffer and measures fill bandwidth.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--forceBlitter Force blitter engine. Test will be skipped if device does not support blitter. Warning: in OpenCL blitter may still be used even if not forced (0 or 1)</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--patternSize Size of the fill pattern</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateCpu|allocates a unified shared memory buffer and measures time to migrate it from GPU to CPU.|<ul><li>--accessAllBytes Select, whether entire resource or only one byte will be accessed on CPU (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedMigrateGpu|allocates a unified shared memory buffer and measures time to migrate it from CPU to GPU.|<ul><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Host or Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteBuffer|allocates an OpenCL buffer and measures write bandwidth. Write operation means transfer from CPU to GPU.|<ul><li>--compressed Select if the buffer is to be compressed. Will be skipped, if device does not support compression (0 or 1)</li><li>--context How context will be created (Root or Tile0 or Tile1 or Tile2 or Tile3 or a list separated with ':')</li><li>--memory Placement of memory for the buffer (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--queue Which device within the context will perform the operation (Root or Tile0 or Tile1 or Tile2 or Tile3)</li><li>--size Size of the buffer</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:x:|:heavy_check_mark:|



# p2p_benchmark
P2P Benchmark is a set of tests aimed at measuring bandwidth and latency of memory transfers between peer devices.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
UsmCopyMultipleBlits|allocates two unified device memory buffers on separate devices and performs a copy between sections (or chunks) of these using a different copy engine and measures bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dstDeviceId Destination device</li><li>--size Size of the operation processed by each engine</li><li>--srcDeviceId Source device</li></ul>|:heavy_check_mark:|:x:|
UsmEUCopy|allocates two unified device memory buffers on separate devices, performs a copy between them using a compute engine, and reports bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy.|<ul><li>--contents Contents of the buffers (Zeros or Random)</li><li>--dstDeviceId Destination device</li><li>--reuseCmdList Command list is reused between iterations (0 or 1)</li><li>--size Size of the buffer</li><li>--srcDeviceId Source device</li><li>--useEvents Perform GPU-side measurements using events (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
UsmImmediateCopyMultipleBlits|allocates two unified device memory buffers on separate devices and performs a copy between sections (or chunks) of these using a different copy engine with an immediate command list and measures bandwidth. Test first checks for P2P capabilities in the target platform before submitting the copy. Results for each individual blitter engine is measured using GPU-based timings and reported separately. Total bandwidths are calculated by dividing the total buffer size by the worst result from all engines. Division of work among blitters is not always even - if main copy engine is specified (rightmost bit in --bliters argument), it gets a half of the buffer and the rest is divided between remaining copy engines. Otherwise the division is even.|<ul><li>--blitters A bit mask for selecting copy engines</li><li>--dstDeviceId Destination device</li><li>--size Size of the operation processed by each engine</li><li>--srcDeviceId Source device</li></ul>|:heavy_check_mark:|:x:|



# record_and_replay_benchmark
Record and Replay Benchmark is a set of tests aimed at measuring performance of recording and execution of commandlist graphs.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
RecordGraph|measures overhead of recording a graph.|<ul><li>--emulate Emulate record and replay graph API using regular commandlists. (0 or 1)</li><li>--mDest Take graph destroy phase into account in perf measurment. (0 or 1)</li><li>--mInst Take graph instantion phase into account in perf measurment. (0 or 1)</li><li>--mRec Take graph recording phase into account in perf measurment. (0 or 1)</li><li>--nAppendCopy Number of appendCopy calls per command set</li><li>--nAppendKern Number of appendLaunchKernel calls per command set.</li><li>--nCmdSetsInLvl Number of command sets per level.</li><li>--nForksInLvl Number of forks to introduce in the graph per graph level.</li><li>--nInstantiations Number of executable graphs to instantiate.</li><li>--nLvls Number of levels (nesting) in the graph using forks.</li></ul>|:heavy_check_mark:|:x:|



# ulls_benchmark
Ulls Benchmark is a set of tests aimed at measuring Ultra Low Latency Submission (ULLS) performance impact.
| Test name | Description | Params | L0 | OCL |
|-----------|-------------|--------|----|-----|
BestSubmission|enqueues a system memory write via PIPE_CONTROL and measures when update becomes visible on the CPU.|<ul></ul>|:heavy_check_mark:|:x:|
BestWalkerNthCommandListSubmission|enqueues single kernel on n command lists, which updates system memory location and then busy-loops on CPU until the update of the kernel of nth command list becomes visible. This is L0 only test.|<ul><li>--CmdListCount Command list count</li></ul>|:heavy_check_mark:|:x:|
BestWalkerNthSubmission|enqueues n kernels, which updates system memory location and then busy-loops on CPU until the update of nth kernel becomes visible.|<ul><li>--KernelCount Kernel count</li></ul>|:heavy_check_mark:|:x:|
BestWalkerNthSubmissionImmediate|enqueues n kernels, which updates system memory location and then busy-loops on CPU until the update of nth kernel becomes visible. Kernel is enqueued using low-latency immediate command list, so the test is LevelZero-specific.|<ul><li>--KernelCount Kernel count</li></ul>|:heavy_check_mark:|:x:|
BestWalkerSubmission|enqueues kernel, which updates system memory location and then busy-loops on CPU until the update becomes visible.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
BestWalkerSubmissionImmediate|enqueues kernel, which updates system memory location and then busy-loops on CPU until the update becomes visible. Kernel is enqueued using low-latency immediate command list, so the test is LevelZero-specific.|<ul></ul>|:heavy_check_mark:|:x:|
BestWalkerSubmissionImmediateMultiCmdlists|Append N kernels on N cmdlists, which updates system memory locations and then waits using busy-loop on CPU until the update becomes visible. Kernels are appended using immediate command lists.Amount of command lists is specified by cmdlistCount.|<ul><li>--cmdlistCount Count of command lists</li></ul>|:heavy_check_mark:|:x:|
CompletionLatency|enqueues system memory write and measures time between the moment, when update is visible on CPU and the moment, when synchronizing call returns.|<ul></ul>|:heavy_check_mark:|:x:|
CopySubmissionEvents|enqueues 4 byte copy to copy engine and return submission delta which is time between host API call and copy engine start|<ul><li>--engine Engine used for copying (RCS or CCS0 or CCS1 or CCS2 or CCS3 or BCS or BCS1 or BCS2 or BCS3 or BCS4 or BCS5 or BCS6 or BCS7 or BCS8)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
EmptyKernel|enqueues empty kernel and measures time to launch it and wait for it on CPU, thus measuring walker spawn time.|<ul><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
EmptyKernelImmediate|enqueues empty kernel and measures time to launch it using immediate command list and wait for it on CPU, thus measuring walker spawn time.|<ul><li>--UseEventForHostSync If true, use events to synchronize with host.If false, use zeCommandListHostSynchronize (0 or 1)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
EmptyKernelsWithGlobalTimer|submits multiple kernels with signal events and barriers in single command lists, uses global timestamps to get total time of their execution|<ul><li>--count Count of kernels</li></ul>|:heavy_check_mark:|:x:|
EnqueueBarrierWithEmptyWaitlist|enqueues kernel with barriers with empty waitlists inbetween, waiting on the last barriers event|<ul><li>--enqueueCount Number of enqueues</li><li>--outOfOrderQueue Use out of order queue (0 or 1)</li></ul>|:x:|:heavy_check_mark:|
KernelSwitch|measures time from end of one kernel till start of next kernel|<ul><li>--barrier synchronization with barrier instead of events (0 or 1)</li><li>--count Count of kernels</li><li>--ctrBasedEvents use counter based events for in order (0 or 1)</li><li>--hostVisible events are with host visible flag (0 or 1)</li><li>--ioq use in order queue/command list (0 or 1)</li><li>--kernelTime Approximately how long a single kernel executes, in us</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
KernelSwitchImm|measures time from end of one kernel till start of next kernel using immediate command lists|<ul><li>--barrier synchronization with barrier instead of events (0 or 1)</li><li>--count Count of kernels</li><li>--cpuVisible events are with host visible flag (0 or 1)</li><li>--ctrEvents use counter based events for in order (0 or 1)</li><li>--ioq use in order queue/command list (0 or 1)</li><li>--kTime Approximately how long a single kernel executes, in us</li><li>--profiling use profiling to obtain switch time (0 or 1)</li></ul>|:heavy_check_mark:|:x:|
KernelWithWork|measures time required to run a GPU kernel which assigns constant values to elements of a buffer. Each thread assigns one value.|<ul><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
KernelWithWorkImmediate|measures time required to run a GPU kernel which assigns constant values to elements of a buffer using immediate command list. Each thread assigns one value.|<ul><li>--UseEventForHostSync If true, use events to synchronize with host.If false, use zeCommandListHostSynchronize (0 or 1)</li><li>--usedIds Which of the get_global_id() and get_local_id() calls will be used in the kernel (None or Global or Local or AtomicPerWkg)</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size (aka local work size)</li></ul>|:heavy_check_mark:|:x:|
KernelWithWorkPeriodic|measures average time required to run a GPU kernel which assigns constant values to elements of a buffer. Each thread assigns one value. Kernel is run multiple times with a set delay between submissions.|<ul><li>--numSubmissions Number of kernel enqueues to run</li><li>--timeBetweenSubmissions Delay between kernel enqueues in microseconds</li></ul>|:heavy_check_mark:|:x:|
MultiKernelExecution|submits multiple kernel in single command lists, measures total time of their execution|<ul><li>--count Count of kernels within command list</li><li>--delay how much delay between atomic reads</li><li>--inOrderOverOOO use out of order queue to implement in order queue (0 or 1)</li><li>--profiling use profiling on the gpu to collect score (0 or 1)</li><li>--useMCL Use mutable command list (0 or 1)</li><li>--wkgCount work group count of each kernel</li><li>--wkgSizes work group size of each kernel</li></ul>|:heavy_check_mark:|:x:|
MultiQueueSubmission|enqueues kernel on multiple command queues|<ul><li>--queueCount Number of command queues created</li><li>--wgc Workgroup count</li><li>--wgs Workgroup size</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
MultipleImmediateWithDependencies|Creates N immediate command lists. Submits kernels in order to each of thoseEach kernel has a dependency on previous oneSubmissions are small to allows concurrent executionMeassures time from scheduling start, till all command lists are completed|<ul><li>--UseEventForHostSync If true, use events to synchronize with host. If false, use zeCommandListHostSynchronize (0 or 1)</li><li>--cmdlistCount Count of command lists</li></ul>|:heavy_check_mark:|:x:|
NewResourcesSubmissionDevice|enqueues kernel that uses a buffer placed in device memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
NewResourcesSubmissionHost|enqueues kernel that uses a buffer placed in host memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
NewResourcesWithGpuAccess|enqueues kernel that accesses an entire buffer placed in device memory to measure resource preparation time. The resource is destroyed and recreated for each iteration to ensure it is a different memory allocation.|<ul><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
QueueConcurrency|Submits multiple kernels to out of order queue returning events. There is a sequence of long kernel, short kernel, wait for short kernel Then calls synchronization and meassures performance|<ul><li>--kernelCount How many kernels are submitted</li><li>--kernelTime How long each work item is in kernel</li><li>--workgroupCount Workgroup Count of each kernel</li></ul>|:x:|:heavy_check_mark:|
QueuePriorities|Uses queues with different priorities to meassure submission and context switch latencies|<ul><li>--highTime How long each work item is in high priority kernel</li><li>--lowTime How long each work item is in low priority kernel</li><li>--priorities Low priority command queue property is used (0 or 1)</li><li>--sleep sleep time in us after low priority kernel flushed</li><li>--wgc Workgroup count of high priority kernel</li></ul>|:x:|:heavy_check_mark:|
QueueSwitch|creates multiple queues, creates dependencies with events between those and measures switch time|<ul><li>--kernelTime Time for each kernel execution</li><li>--switchCount How many switches form each iteration</li></ul>|:heavy_check_mark:|:x:|
RoundTripSubmission|enqueues kernel which updates system memory location and waits for it with a synchronizing API.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedFirstCpuAccess|allocates a unified shared memory buffer and measures time to access it on CPU after creation.|<ul><li>--initialPlacement Hint for initial placement of the resource passed to the driver (Any or Host or Device)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
UsmSharedFirstGpuAccess|allocates a unified shared memory buffer and measures time to access it on GPU after creation.|<ul><li>--initialPlacement Hint for initial placement of the resource passed to the driver (Any or Host or Device)</li><li>--size Size of the buffer</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WalkerCompletionLatency|enqueues a kernel writing to system memory and measures time between the moment when update is visible on CPU and the moment when synchronizing call returns|<ul><li>--inOrderQueue If set use IOQ, otherwise OOQ. Applicable only for OCL. (0 or 1)</li><li>--useFence Use fence during submission and for further completion. (0 or 1)</li></ul>|:heavy_check_mark:|:heavy_check_mark:|
WalkerSubmissionEvents|enqueues an empty kernel with GPU-side profiling and checks delta between queue time and start time.|<ul></ul>|:heavy_check_mark:|:heavy_check_mark:|
WriteLatency|unblocks event on GPU, then waits for timestamp being written.|<ul></ul>|:heavy_check_mark:|:x:|



